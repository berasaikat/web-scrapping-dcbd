{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spazer tool for processing web pages\n",
    "\n",
    "```\n",
    "Team members:  \n",
    "\n",
    "- Roudranil Das  \n",
    "   - MDS202227\n",
    "   - roudranil@cmi.ac.in\n",
    "- Saikat Bera \n",
    "   - MDS202228\n",
    "   - saikatb@cmi.ac.in\n",
    "- Shreyan Chakraborty \n",
    "   - MDS202237\n",
    "   - shreyanc@cmi.ac.in\n",
    "- Soham Sengupta \n",
    "   - MDS202241\n",
    "   - sohams@cmi.ac.in\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Spazer\n",
      "\n",
      "Reading 0.html\n",
      "Writing reduced 0.html\n",
      "Reading 1.html\n",
      "Writing reduced 1.html\n",
      "Reading 2.html\n",
      "Writing reduced 2.html\n",
      "Reading 3.html\n",
      "Writing reduced 3.html\n",
      "Reading 4.html\n",
      "Writing reduced 4.html\n",
      "324095\n",
      "\n",
      "Total Space used by input files = 324095 characters.\n",
      "Total Space used by output files = 7769 characters.\n",
      "Total Space Gained = 97.6%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "import requests\n",
    "\n",
    "# utility function to remove overlapping slices in a list of slices\n",
    "def remove_overlapping(slices):\n",
    "    slices = sorted(slices, key=lambda x: x[0]) # sort according to starting points\n",
    "    i = 0\n",
    "    cleaned_slices = [slices[0]] # adding the first slice to the cleaned list\n",
    "    # iterating over the next slices\n",
    "    for next_slice in slices[1:]:\n",
    "        if cleaned_slices[i][1] < next_slice[0]:\n",
    "            # if the next slice starts after the current one ends, then atleast the starting point of the next slice wil be considered, so append that and increment i\n",
    "            cleaned_slices.append(next_slice)\n",
    "            i += 1\n",
    "            continue\n",
    "        elif next_slice[0] <= cleaned_slices[i][1] < next_slice[1]:\n",
    "            # if the end of the current slice is strictly in between the next slice then update the current end point to the end point of the next slice\n",
    "            # current[0] <= next[0] <= current[1] < next[1]\n",
    "            cleaned_slices[i] = (cleaned_slices[i][0], next_slice[1])\n",
    "            continue\n",
    "        elif cleaned_slices[i][1] >= next_slice[1]:\n",
    "            # if the next slice lies completely within the current slice, then dont do anything, go to the next slice\n",
    "            # every slice in slices is sorted in asceding order of starting value\n",
    "            # current[0] <= next[0] < next[1] <= current[1]\n",
    "            continue\n",
    "\n",
    "    return cleaned_slices\n",
    "\n",
    "#Variables to track the input, output and gained space\n",
    "space_gained = 0\n",
    "space_input = 0\n",
    "space_output = 0\n",
    "\n",
    "PINCODE_OR_EMAIL = re.compile(r\" (\\d ?){6}([a-zA-Z]?|.?)|\\[at\\]|([a-zA-Z0-9]|\\.|\\_)@([a-zA-Z0-9]|\\.|\\_)|\\[dot\\]\")\n",
    "\n",
    "print(\"Welcome to Spazer\\n\")\n",
    "\n",
    "num_input = len(glob.glob1(\"input\", \"*.html\"))\n",
    "\n",
    "# downloading and reading stopwords from github gist\n",
    "# needs internet connection\n",
    "currencies = \"https://gist.githubusercontent.com/Roudranil/38d716839b75ad65a83376f29f9331bd/raw/a3630dfa70544e4d0fda4487f853041e9ed42dc4/StopWords_Currencies.txt\"\n",
    "genericlong = \"https://gist.githubusercontent.com/Roudranil/db48ab9424912f4eef5e39ef4071bee8/raw/5084c81c9e914403933b7f9b784944e0786c13fb/StopWords_GenericLong.txt\"\n",
    "generic = \"https://gist.githubusercontent.com/Roudranil/3fc4fe737b04f851e10684fa86fe6144/raw/eb74b0f6a62976b463644d6e03234bfcfcf3fd5f/StopWords_Generic.txt\"\n",
    "nltk = \"https://gist.githubusercontent.com/Roudranil/8a60820f0046d10f9410167dc837681d/raw/96af88fc76937c6f29e54881a632eb42421a9071/StopWords_nltk.txt\"\n",
    "\n",
    "STOPWORDS = (\n",
    "    [x.split(\"|\")[0].strip() for x in requests.get(currencies).text.lower().strip(\"\\n\").split(\"\\n\")] + \\\n",
    "    requests.get(genericlong).text.lower().strip(\"\\n\").split(\"\\n\") + \\\n",
    "    requests.get(generic).text.lower().strip(\"\\n\").split(\"\\n\") + \\\n",
    "    requests.get(nltk).text.strip(\"\\n\").split(\"\\n\")\n",
    ")\n",
    "\n",
    "# # use this code to read the stopwords in case above method is not working\n",
    "# # needs the stopwords text files to be downloaded and placed in the StopWords folder in the cwd\n",
    "# STOPWORDS = (\n",
    "#     [x.split(\"|\")[0].strip() for x in open(\"StopWords/StopWords_Currencies.txt\").read().lower().strip(\"\\n\").split(\"\\n\")] + \\\n",
    "#     open(\"StopWords/StopWords_Generic.txt\").read().lower().strip(\"\\n\").split(\"\\n\") + \\\n",
    "#     open('StopWords/StopWords_GenericLong.txt').read().lower().strip(\"\\n\").split(\"\\n\") + \\\n",
    "#     open(\"StopWords/StopWords_nltk.txt\", \"r\").read().strip(\"\\n\").split(\"\\n\")\n",
    "# )\n",
    "\n",
    "for x in range(num_input):\n",
    "    filename = str(x) + \".html\"\n",
    "    file = os.path.join('input', filename)\n",
    "    # file = pathlib.Path('input/' + filename)\n",
    "    if (pathlib.Path(file).exists()):\n",
    "\n",
    "        #Read each file\n",
    "        print(\"Reading \" + filename)\n",
    "        f = open(file, 'r', errors=\"ignore\")\n",
    "        contents = f.read()   \n",
    "        \n",
    "        #Remove html tags\n",
    "        soup = BeautifulSoup(contents, 'lxml')        \n",
    "        output = soup.get_text() \n",
    "        \n",
    "        #Your code begins  \n",
    "        #################################\n",
    "\n",
    "        # extracts every single line of the website, but after stripping it\n",
    "        output = [x.strip() for x in output.split('\\n') if x.strip() != '']\n",
    "\n",
    "        # removes stopwords in every line, and substitutes some characters like |\n",
    "        for i, line in enumerate(output):\n",
    "            output[i] = ' '.join([word for word in line.split(' ') if word.lower() not in STOPWORDS])\n",
    "        output = ' \\n '.join(output).replace(\"|\", \"\").replace(\"-\", \" - \")\n",
    "\n",
    "        # takes slices of the output where pincode or email is matched\n",
    "        slices = []\n",
    "        for match in re.finditer(PINCODE_OR_EMAIL, output):\n",
    "            s = match.start()\n",
    "            e = match.end()\n",
    "            slices.append((s-201, e+100))\n",
    "\n",
    "        slices = remove_overlapping(slices) # overlapping slices are removed\n",
    "        output = \"\\n\\n\".join([output[s[0]:s[1]] for s in slices])\n",
    "\n",
    "        #Your code ends  \n",
    "        #################################              \n",
    "\n",
    "        #Write the output variable contents to output/ folder.\n",
    "        print (\"Writing reduced \" + filename)\n",
    "        fw = open('output/' + filename, \"w\")\n",
    "        fw.write(output)\n",
    "        fw.close()\n",
    "        f.close()\n",
    "        \n",
    "        #Calculate space savings\n",
    "        space_input = space_input + len(contents)\n",
    "        space_output = space_output + len(output)\n",
    "\n",
    "print(space_input)        \n",
    "space_gained = round((space_input - space_output) * 100 / space_input, 2)\n",
    "\n",
    "print(\"\\nTotal Space used by input files = \" + str(space_input) + \" characters.\") \n",
    "print(\"Total Space used by output files = \" + str(space_output) + \" characters.\")\n",
    "print(\"Total Space Gained = \" + str(space_gained) + \"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
